{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e90fe518",
   "metadata": {},
   "source": [
    "# Working with data frames continued \n",
    "## Week 5 \n",
    "\n",
    "Two very important concepts introduced this week around working with data frames are joins and reshaping your data structure. In this notebook we will look at some worked examples of both of these concepts in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46328fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as always, first load packages and modules \n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2defd334",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe called patient_details \n",
    "patient_details = pd.DataFrame.from_dict({\"id\": [1, 2, 3, 4, 5],           \n",
    "                                          \"age\": [40, 56, 23, 45, 34]})\n",
    "\n",
    "patient_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216a6cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a new dataframe called appointment details \n",
    "## notice only IDs 3, 4, and 5 exist in both data sets \n",
    "appt_details = pd.DataFrame.from_dict({\"id\": [3, 4, 5, 6, 7],\n",
    "                                       \"month\": [\"March\", \"May\", \"June\", \"June\", \"April\"],\n",
    "                                       \"return_visit\": [True, False, True, True, False]})\n",
    "\n",
    "appt_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fb333",
   "metadata": {},
   "source": [
    "Now lets join these two data sets to create a data set for analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918ab74",
   "metadata": {},
   "source": [
    "### Inner join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99720ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join \n",
    "\n",
    "analysis_inner = pd.merge(patient_details, appt_details, \n",
    "                          how = \"inner\", \n",
    "                          on = \"id\")\n",
    "\n",
    "analysis_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6474a94",
   "metadata": {},
   "source": [
    "After an inner join, only IDs `3`, `4`, and `5` are returned as they are the only matching rows in both data frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e22ef",
   "metadata": {},
   "source": [
    "### Full join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0bad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# full join \n",
    "\n",
    "analysis_full = pd.merge(patient_details, appt_details, \n",
    "                         how = \"outer\", # full join is denoted with outer  \n",
    "                         on = \"id\")\n",
    "\n",
    "analysis_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf341c27",
   "metadata": {},
   "source": [
    "In a full join, as you can see, there are `NaN` values for IDs `1` and `2` in the `month` and `return_visit` columns as these IDs are not in the `appt_details` data frame. And there are `NaN` values for IDs `6` and `7` for in the `age` column as these IDs are not in the `patient_details` data frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdff64c",
   "metadata": {},
   "source": [
    "### Left join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4aab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join \n",
    "\n",
    "analysis_left = pd.merge(patient_details, appt_details, \n",
    "                         how = \"left\", \n",
    "                         on = \"id\")\n",
    "\n",
    "analysis_left "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c89cfb",
   "metadata": {},
   "source": [
    "In a left join, all of the data from `patient_details` was kept, resulting in `NaN` values for IDs `1` and `2` in the columns `month` and `return_visit` as these IDs are not in the `appt_details` data frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3443caca",
   "metadata": {},
   "source": [
    "### Right join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99a45d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# right join \n",
    "\n",
    "analysis_right = pd.merge(patient_details, appt_details,\n",
    "                          how = \"right\", \n",
    "                          on = \"id\")\n",
    "\n",
    "analysis_right "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beab0a1",
   "metadata": {},
   "source": [
    "A right join is essentially the opposite of a left join, keeping all of the data from `appt_details`. Thus, IDs `6` and `7` have `NaN` for `age` as these IDs are not in the `patient_details` data frame. \n",
    "\n",
    "If you switch which data frames are on the right and left, a right join and left join will produce the same result, just with columns in a different order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join with order of data frame inputs switched is the same as the right join above \n",
    "\n",
    "analysis_left_switch = pd.merge(appt_details, patient_details, \n",
    "                                how = \"left\", \n",
    "                                on = \"id\")\n",
    "\n",
    "analysis_left_switch \n",
    "# which is the same as analysis_right only age is the last column rather than the 2nd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f647414",
   "metadata": {},
   "source": [
    "## But what about when the linkage keys are not called the same thing? \n",
    "\n",
    "In this case, we need to specify the arguments `left_on` for the left-hand side dataset and `right_on` for the right-hand side dataset. Let's look at a full join as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5738dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder of what the columns are called in appt details \n",
    "\n",
    "appt_details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f4d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a 2nd patient details df with a different name for the id variable \n",
    "\n",
    "patient_details2 = pd.DataFrame.from_dict({\"patient\": [1, 2, 3, 4, 5],           \n",
    "                                          \"age\": [40, 56, 23, 45, 34]})\n",
    "\n",
    "patient_details2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b26054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_full2 = pd.merge(patient_details2, appt_details, \n",
    "                          how = \"outer\", \n",
    "                          left_on = \"patient\", \n",
    "                          right_on = \"id\")\n",
    "\n",
    "analysis_full2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b979fdf",
   "metadata": {},
   "source": [
    "As you can see, `analysis_full2` and `analysis_full` are the same in content, though when using `left_on` and `right_on`, both original data frame linked key columns are retained. If you prefer to have the merged data frame with a single ID or linking key column, you can change the name of the variables in the data frames to be merged to match first then perform your merge as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e539f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patient_details2.rename({\"patient\": \"id\"}, \n",
    "                        axis = 1).merge(appt_details, \n",
    "                                        how = \"outer\", \n",
    "                                        on = \"id\")\n",
    "\n",
    "# this does not rename patient to id in the patient_details2 df, it just does so for the merge operation\n",
    "# notice we pass a dictionary structure to the rename method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac31cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# patient_details2 still has ID column called patient\n",
    "## becuase in the code above we did not change the stored data object \n",
    "\n",
    "patient_details2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70742f",
   "metadata": {},
   "source": [
    "To merge by multiple keys, you just need to pass the keys in a list to `pd.merge`. \n",
    "So for example: `pd.merge(df1, df2, how = \"left\", on = [\"A\", \"B\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2cd434",
   "metadata": {},
   "source": [
    "## Reshaping data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc073e5",
   "metadata": {},
   "source": [
    "To move between wide and long data formats in `pandas` we use the `pd.pivot()` and `pd.melt()` functions. \n",
    "\n",
    "* `pd.pivot(df, index = , columns = , values =)`: reshape the data from long to wide, where \n",
    "    - `columns` are the columns used to make the new data frame's columns \n",
    "    - `values` are the columns to use for populating the new data frame's values \n",
    "    - `index` is the column(s) used to make the new data frame's index. If not specified, uses the existing index by default \n",
    "    \n",
    "* `pd.melt(df, id_vars = , value_vars = , var_name = , value_name = )`: reshape the data from wide to long, where \n",
    "    - `id_vars` are the columns used as identifier variables \n",
    "    - `value_vars` are the column(s) to unpivot or make longer\n",
    "    - `var_name` is the name used for the `variable` column\n",
    "    - `value_name` is the name used for the `value` column \n",
    "    - `ignore_index` if `True`, original index is ignored. If `False`, the original index is retained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe4791",
   "metadata": {},
   "source": [
    "To strengthen our understanding of wide and long formats of data, we will again return the `gapminder` data that we have seen a few times before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14db9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remember to change the file path to the data to suit your own set up \n",
    "gap_data = pd.read_csv(\"../data/gapminder_data.csv\")\n",
    "\n",
    "gap_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d93b2",
   "metadata": {},
   "source": [
    "Sometimes, as with the  gapminder dataset, we have multiple types of observed data. It is somewhere in between the purely long and wide data formats:\n",
    "\n",
    "* 3 \"ID variables\": `continent`, `country`, `year`\n",
    "* 3 \"observation variables\": `pop` `lifeExp`, `gdpPercap`\n",
    "\n",
    "It is  pretty common to have data in this format in most cases despite not having ALL observations in 1 column, since all 3 observation variables have different units. Depending on your question being asked of the data, this can be (and often is) considered a tidy format. But we can play with switching it to purely long and wide formats to show what that means (i.e., long would be 4 ID variables and 1 observation variable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23852216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data longer\n",
    "# notice we are using the list data structure to pass multiple columns to some arguments \n",
    "\n",
    "gap_data_long = gap_data.melt(id_vars = [\"country\", \"continent\", \"year\"],\n",
    "                              value_vars = [\"lifeExp\", \"pop\", \"gdpPercap\"],\n",
    "                              var_name = \"metric\")\n",
    "\n",
    "gap_data_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you do not necessarily need to specify the value_vars parameter \n",
    "## but it is good practice to be explicit in your code \n",
    "\n",
    "# same outcome as above\n",
    "gap_data.melt(id_vars = [\"country\", \"continent\", \"year\"],\n",
    "              var_name = \"metric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd71e9",
   "metadata": {},
   "source": [
    "Using our new `gap_data_long` data frame, we can convert it back be more like the original `gap_data` format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f055d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wide \n",
    "\n",
    "gap_data_wide = gap_data_long.pivot(columns = \"metric\",\n",
    "                                    values = \"value\",\n",
    "                                    index = [\"country\", \"continent\", \"year\"]).reset_index()\n",
    "\n",
    "gap_data_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we remove the reset_index, our resulting df will have a multilevel index\n",
    "## which could indeed be what you are interested in! \n",
    "\n",
    "wide_mindex = gap_data_long.pivot(columns = \"metric\",\n",
    "                                  values = \"value\",\n",
    "                                  index = [\"country\", \"continent\", \"year\"])\n",
    "\n",
    "wide_mindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets compare the indexes from gap_data_wide and wide_mindex \n",
    "print(gap_data_wide.index) # range index \n",
    "\n",
    "print(wide_mindex.index) # multi index "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6befe531",
   "metadata": {},
   "source": [
    "We could also make the data even wider by passing multiple columns to the `values` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60a995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wider data \n",
    "\n",
    "## again without resetting the index we will auotmatically have a MultiIndex \n",
    "### with country and continent as we are passing a list to the index parameter \n",
    "gap_data_wide2 = gap_data.pivot(columns = \"year\",\n",
    "                                values = [\"lifeExp\", \"pop\", \"gdpPercap\"],\n",
    "                                index = [\"country\", \"continent\"])\n",
    "\n",
    "gap_data_wide2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29687e0",
   "metadata": {},
   "source": [
    "Let's compare the dimensions of our tibbles: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc9cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_data.shape # 1704 rows and 6 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_data_long.shape  # 5112 rows by 5 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0accbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_data_wide.shape # 1704 rows and 6 columns - same as gap_data as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf910bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_mindex.shape # 1704 rows by 3 columns - becuase we have a MultiIndex with 3 columns! (hence 3 less columns than gap_data_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbdda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_data_wide2.shape # 142 rows and 36 columns\n",
    "## would be 142 rows by 38 columns if we reset the index - try it out and see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0588031",
   "metadata": {},
   "source": [
    "As we can see by the dimensions, `gap_data_long` is indeed skinnier (fewer columns) and longer (many more rows) than the other data frames and `gap_data_wide2` is indeed much wider (many more columns and fewer rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbbb6b",
   "metadata": {},
   "source": [
    "### Multi-level indexes \n",
    "\n",
    "When working with multi-index data frames, we cannot use `df.pivot()` or `df.melt()` but must rather employ the `df.unstack()` and `df.stack()` methods. \n",
    "\n",
    "* `df.unstack(level = -1, fill_value = None)` is the MultiIndex equivalent to `df.pivot()`\n",
    "    - `level` is default, it is set to `-1` (i.e., the last level can be unstacked). If we pass the specified level, it will unstack those levels. \n",
    "    - `fill_value` are the values you wish to replace resulting `NaN` values with in the new data frame, by default set to `None` \n",
    "\n",
    "\n",
    "* `df.stack(level = -1 , dropna = True)` is the MultiIndex equivalent to `df.melt()`\n",
    "    - `level` is level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels. By default set to `-1` (i.e., the last level can be stacked)\n",
    "    - `dropna` is used to manage the rows with resulting `NaN` values. Default is `True` which drops rows in the resulting Frame/Series with all missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566148c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a same example df with a multi-level index to better see this in action \n",
    "\n",
    "multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n",
    "                                       ('weight', 'pounds')])\n",
    "\n",
    "df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\n",
    "                                    index = ['cat', 'dog'],\n",
    "                                    columns = multicol1)\n",
    "\n",
    "df_multi_level_cols1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca03e8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# stack the data frame which has a multi-level column axis \n",
    "\n",
    "stack1 = df_multi_level_cols1.stack()\n",
    "\n",
    "stack1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f406be8",
   "metadata": {},
   "source": [
    "We can also unstack these columns with the sister function `df.unstack()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aece50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack \n",
    "\n",
    "unstack1 = stack1.unstack()\n",
    "\n",
    "unstack1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f20317",
   "metadata": {},
   "source": [
    "It is common to have missing values when stacking a dataframe with multi-level columns, as the stacked dataframe typically has more values than the original dataframe. Missing values are filled with `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8eb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n",
    "                                        ('height', 'm')])\n",
    "\n",
    "df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\n",
    "                                     index = ['cat', 'dog'],\n",
    "                                     columns = multicol2)\n",
    "\n",
    "df_multi_level_cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad302982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stack2 = df_multi_level_cols2.stack()\n",
    "\n",
    "stack2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1931ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack \n",
    "unstack2 = stack2.unstack()\n",
    "\n",
    "unstack2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06f1c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# stack by column axis 0 instead of default -1 \n",
    "\n",
    "stack3 = df_multi_level_cols2.stack(level = 0)\n",
    "\n",
    "stack3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack \n",
    "\n",
    "unstack3 = stack3.unstack()\n",
    "\n",
    "unstack3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd95fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare to unstack3 above \n",
    "\n",
    "unstack3_2 = stack3.unstack(level = 0)\n",
    "\n",
    "unstack3_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d9345",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## You did it! üéâ \n",
    "\n",
    "Well done for making it to the end of this notebook. If you have not done so yet, move to the RMarkdwon notebook next. \n",
    "\n",
    "‚≠ê‚≠ê‚≠ê‚ùìüë£ Do not forget your 3 stars, a wish, and a step mini-diaries once you have completed the content for this week. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b0a122",
   "metadata": {},
   "source": [
    "---\n",
    "*Dr. Brittany Blankinship (2024)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
